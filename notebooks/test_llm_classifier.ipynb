{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLMClassifier Test\n",
        "\n",
        "Test the LLMClassifier on quark/gluon jet data using OpenRouter API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
        "\n",
        "from vibe_jet_tagging import LLMClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "\n",
        "Load the quark/gluon jet dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10000 jets\n",
            "X shape: (10000, 139, 4)\n",
            "y shape: (10000,)\n",
            "Quark jets: 5074\n",
            "Gluon jets: 4926\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "data_path = Path.cwd().parent / 'data' / 'qg_jets.npz'\n",
        "data = np.load(data_path)\n",
        "\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "\n",
        "print(f\"Loaded {len(X)} jets\")\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"Quark jets: {(y == 1).sum()}\")\n",
        "print(f\"Gluon jets: {(y == 0).sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize LLMClassifier\n",
        "\n",
        "Set up the classifier with OpenRouter API.\n",
        "\n",
        "**Note:** Set your OpenRouter API key as an environment variable:\n",
        "```bash\n",
        "export OPENROUTER_API_KEY=\"your-key-here\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: OPENROUTER_API_KEY not set. Please set it before running predictions.\n",
            "export OPENROUTER_API_KEY='your-key-here'\n"
          ]
        }
      ],
      "source": [
        "# Check if API key is set\n",
        "if 'OPENROUTER_API_KEY' not in os.environ:\n",
        "    print(\"WARNING: OPENROUTER_API_KEY not set. Please set it before running predictions.\")\n",
        "    print(\"export OPENROUTER_API_KEY='your-key-here'\")\n",
        "else:\n",
        "    print(\"âœ“ API key found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize classifier\n",
        "clf = LLMClassifier(\n",
        "    model_name=\"anthropic/claude-3.5-sonnet\",\n",
        "    template_name=\"simple_list\",\n",
        "    format_type=\"list\",\n",
        "    templates_dir=str(Path.cwd().parent / 'templates')\n",
        ")\n",
        "\n",
        "# Fit (no-op for zero-shot)\n",
        "clf.fit([], [])\n",
        "\n",
        "print(\"Classifier initialized\")\n",
        "print(f\"Model: {clf.model_name}\")\n",
        "print(f\"Template: {clf.template_name}\")\n",
        "print(f\"Format: {clf.format_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Single Jet Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on a single jet\n",
        "test_jet = X[0]\n",
        "true_label = y[0]\n",
        "\n",
        "print(f\"True label: {true_label} ({'quark' if true_label == 1 else 'gluon'})\")\n",
        "print(f\"\\nJet shape: {test_jet.shape}\")\n",
        "print(f\"Number of particles (pt > 0): {(test_jet[:, 0] > 0).sum()}\")\n",
        "\n",
        "# Make prediction\n",
        "prediction = clf.predict([test_jet])[0]\n",
        "print(f\"\\nPredicted label: {prediction} ({'quark' if prediction == 1 else 'gluon'})\")\n",
        "print(f\"Correct: {prediction == true_label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on 100 Jets\n",
        "\n",
        "Run the classifier on 100 jets and compute metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select 100 jets\n",
        "n_test = 100\n",
        "X_test = X[:n_test]\n",
        "y_test = y[:n_test]\n",
        "\n",
        "print(f\"Testing on {n_test} jets...\")\n",
        "print(f\"True distribution: {(y_test == 1).sum()} quark, {(y_test == 0).sum()} gluon\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions (this will take a while)\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "predictions = []\n",
        "for i, jet in enumerate(tqdm(X_test)):\n",
        "    pred = clf.predict([jet])[0]\n",
        "    predictions.append(pred)\n",
        "    \n",
        "    # Print progress every 10 jets\n",
        "    if (i + 1) % 10 == 0:\n",
        "        acc = accuracy_score(y_test[:i+1], predictions)\n",
        "        print(f\"After {i+1} jets: Accuracy = {acc:.3f}\")\n",
        "\n",
        "predictions = np.array(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "auc = roc_auc_score(y_test, predictions)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"AUC Score: {auc:.3f}\")\n",
        "print(f\"\\nPredicted distribution: {(predictions == 1).sum()} quark, {(predictions == 0).sum()} gluon\")\n",
        "print(f\"True distribution: {(y_test == 1).sum()} quark, {(y_test == 0).sum()} gluon\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"                Predicted\")\n",
        "print(\"                Gluon  Quark\")\n",
        "print(f\"True  Gluon     {cm[0,0]:5d}  {cm[0,1]:5d}\")\n",
        "print(f\"      Quark     {cm[1,0]:5d}  {cm[1,1]:5d}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
