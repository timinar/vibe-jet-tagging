{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM Classifier Test\n",
    "\n",
    "Test the LocalLLMClassifier on quark/gluon jet data using local OpenAI-compatible API.\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Zero-shot jet classification with GPT-OSS-120B (or any reasoning model)\n",
    "- Reasoning effort control (low, medium, high)\n",
    "- Async vs sequential processing comparison\n",
    "- Token usage tracking (local server, no cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from vibe_jet_tagging.local_llm_classifier import LocalLLMClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Local Server\n",
    "\n",
    "Make sure your local LLM server is running.\n",
    "\n",
    "Example with vLLM:\n",
    "```bash\n",
    "vllm serve openai/gpt-oss-120b --port 8000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Check if server is running\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"✅ Local LLM server is running\")\n",
    "        print(f\"Available models: {[m['id'] for m in models['data']]}\")\n",
    "    else:\n",
    "        print(\"⚠️  Server responded but with error\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Local LLM server not reachable\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nStart the server with: vllm serve openai/gpt-oss-120b --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the quark/gluon jet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path.cwd().parent / 'data' / 'qg_jets.npz'\n",
    "data = np.load(data_path)\n",
    "\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "print(f\"Loaded {len(X)} jets\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Quark jets: {(y == 1).sum()}\")\n",
    "print(f\"Gluon jets: {(y == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LocalLLMClassifier\n",
    "\n",
    "Set up the classifier with local OpenAI-compatible API.\n",
    "\n",
    "**Configuration:**\n",
    "- `model_name`: The model identifier (must match server)\n",
    "- `reasoning_effort`: Controls reasoning depth (\"low\", \"medium\", \"high\")\n",
    "- `base_url`: URL of your local server\n",
    "- `api_key`: \"EMPTY\" for local servers without auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier with local API\n",
    "clf = LocalLLMClassifier(\n",
    "    model_name=\"openai/gpt-oss-120b\",\n",
    "    template_name=\"simple_list\",\n",
    "    format_type=\"list\",\n",
    "    templates_dir=str(Path.cwd().parent / 'templates'),\n",
    "    reasoning_effort=\"medium\",   # Options: \"low\", \"medium\", \"high\"\n",
    "    reasoning_summary=\"auto\",    # Options: \"auto\", \"concise\", \"detailed\"\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "\n",
    "# Fit (no-op for zero-shot)\n",
    "clf.fit([], [])\n",
    "\n",
    "print(\"Classifier initialized\")\n",
    "print(f\"Model: {clf.model_name}\")\n",
    "print(f\"Template: {clf.template_name}\")\n",
    "print(f\"Format: {clf.format_type}\")\n",
    "print(f\"Reasoning effort: {clf.reasoning_effort}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Jet Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single jet\n",
    "test_jet = X[0]\n",
    "true_label = y[0]\n",
    "\n",
    "print(f\"True label: {true_label} ({'quark' if true_label == 1 else 'gluon'})\")\n",
    "print(f\"\\nJet shape: {test_jet.shape}\")\n",
    "print(f\"Number of particles (pt > 0): {(test_jet[:, 0] > 0).sum()}\")\n",
    "\n",
    "# Make prediction (sequential mode for single jet)\n",
    "prediction = clf.predict([test_jet], verbose=True, use_async=False)[0]\n",
    "print(f\"\\nPredicted label: {prediction} ({'quark' if prediction == 1 else 'gluon'})\")\n",
    "print(f\"Correct: {prediction == true_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the prompt\n",
    "clf.preview_prompt(test_jet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning Effort Control\n",
    "\n",
    "Test how different reasoning effort levels affect performance and token usage.\n",
    "\n",
    "For reasoning models:\n",
    "- **low**: Fast reasoning with minimal depth (~1,250 tokens)\n",
    "- **medium**: Balanced reasoning (~1,760 tokens)\n",
    "- **high**: Deep reasoning with maximum effort (~7,540 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different reasoning efforts on a single jet\n",
    "test_jet = X[0]\n",
    "\n",
    "efforts = [\"low\", \"medium\", \"high\"]\n",
    "results = []\n",
    "\n",
    "for effort in efforts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing with reasoning_effort={effort}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    clf_test = LocalLLMClassifier(\n",
    "        model_name=\"openai/gpt-oss-120b\",\n",
    "        template_name=\"simple_list\",\n",
    "        format_type=\"list\",\n",
    "        templates_dir=str(Path.cwd().parent / 'templates'),\n",
    "        reasoning_effort=effort,\n",
    "        base_url=\"http://localhost:8000/v1\",\n",
    "        api_key=\"EMPTY\"\n",
    "    )\n",
    "    clf_test.fit([], [])\n",
    "    \n",
    "    start = time.time()\n",
    "    pred = clf_test.predict([test_jet], verbose=True, use_async=False)[0]\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'effort': effort,\n",
    "        'prediction': pred,\n",
    "        'reasoning_tokens': clf_test.total_reasoning_tokens,\n",
    "        'total_tokens': clf_test.total_prompt_tokens + clf_test.total_completion_tokens + clf_test.total_reasoning_tokens,\n",
    "        'time': elapsed\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Effort':<10} {'Reasoning':<12} {'Total':<12} {'Time':<10} {'Pred'}\")\n",
    "print(\"-\" * 70)\n",
    "for r in results:\n",
    "    print(f\"{r['effort']:<10} {r['reasoning_tokens']:<12,} {r['total_tokens']:<12,} {r['time']:<10.2f}s {r['prediction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async vs Sequential Comparison\n",
    "\n",
    "Compare async (concurrent) vs sequential processing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select test jets\n",
    "n_test = 20\n",
    "X_test = X[:n_test]\n",
    "y_test = y[:n_test]\n",
    "\n",
    "print(f\"Testing on {n_test} jets...\")\n",
    "print(f\"True distribution: {(y_test == 1).sum()} quark, {(y_test == 0).sum()} gluon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ASYNC mode (default, much faster)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ASYNC MODE (Concurrent Processing)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "clf_async = LocalLLMClassifier(\n",
    "    model_name=\"openai/gpt-oss-120b\",\n",
    "    template_name=\"simple_list\",\n",
    "    format_type=\"list\",\n",
    "    templates_dir=str(Path.cwd().parent / 'templates'),\n",
    "    reasoning_effort=\"medium\",\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "clf_async.fit([], [])\n",
    "\n",
    "start = time.time()\n",
    "predictions_async = clf_async.predict(X_test, verbose=False, use_async=True)\n",
    "time_async = time.time() - start\n",
    "\n",
    "accuracy_async = accuracy_score(y_test, predictions_async)\n",
    "print(f\"\\nTime: {time_async:.2f}s ({time_async/n_test:.2f}s per jet)\")\n",
    "print(f\"Accuracy: {accuracy_async:.3f}\")\n",
    "print(f\"Total tokens: {clf_async.total_prompt_tokens + clf_async.total_completion_tokens + clf_async.total_reasoning_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SEQUENTIAL mode (slower, for comparison)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEQUENTIAL MODE (One at a time)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "clf_seq = LocalLLMClassifier(\n",
    "    model_name=\"openai/gpt-oss-120b\",\n",
    "    template_name=\"simple_list\",\n",
    "    format_type=\"list\",\n",
    "    templates_dir=str(Path.cwd().parent / 'templates'),\n",
    "    reasoning_effort=\"medium\",\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "clf_seq.fit([], [])\n",
    "\n",
    "start = time.time()\n",
    "predictions_seq = clf_seq.predict(X_test, verbose=False, use_async=False)\n",
    "time_seq = time.time() - start\n",
    "\n",
    "accuracy_seq = accuracy_score(y_test, predictions_seq)\n",
    "print(f\"\\nTime: {time_seq:.2f}s ({time_seq/n_test:.2f}s per jet)\")\n",
    "print(f\"Accuracy: {accuracy_seq:.3f}\")\n",
    "print(f\"Total tokens: {clf_seq.total_prompt_tokens + clf_seq.total_completion_tokens + clf_seq.total_reasoning_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nAsync mode:      {time_async:.2f}s ({time_async/n_test:.2f}s per jet)\")\n",
    "print(f\"Sequential mode: {time_seq:.2f}s ({time_seq/n_test:.2f}s per jet)\")\n",
    "print(f\"\\nSpeedup: {time_seq/time_async:.1f}x faster with async\")\n",
    "print(f\"\\nAccuracy (async):     {accuracy_async:.3f}\")\n",
    "print(f\"Accuracy (sequential): {accuracy_seq:.3f}\")\n",
    "print(f\"\\nPredictions match: {np.array_equal(predictions_async, predictions_seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance on 100 Jets\n",
    "\n",
    "Run the classifier on more jets and compute detailed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 jets for evaluation\n",
    "n_eval = 100\n",
    "X_eval = X[:n_eval]\n",
    "y_eval = y[:n_eval]\n",
    "\n",
    "print(f\"Evaluating on {n_eval} jets...\")\n",
    "print(f\"True distribution: {(y_eval == 1).sum()} quark, {(y_eval == 0).sum()} gluon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using async (fastest)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "clf_eval = LocalLLMClassifier(\n",
    "    model_name=\"openai/gpt-oss-120b\",\n",
    "    template_name=\"simple_list\",\n",
    "    format_type=\"list\",\n",
    "    templates_dir=str(Path.cwd().parent / 'templates'),\n",
    "    reasoning_effort=\"medium\",\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "clf_eval.fit([], [])\n",
    "\n",
    "print(\"Making predictions (async mode)...\")\n",
    "start = time.time()\n",
    "predictions = clf_eval.predict(X_eval, verbose=False, use_async=True)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print(f\"\\nCompleted in {elapsed:.2f}s ({elapsed/n_eval:.2f}s per jet)\")\n",
    "print(f\"Total tokens used: {clf_eval.total_prompt_tokens + clf_eval.total_completion_tokens + clf_eval.total_reasoning_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_eval, predictions)\n",
    "auc = roc_auc_score(y_eval, predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"AUC Score: {auc:.3f}\")\n",
    "print(f\"\\nPredicted distribution: {(predictions == 1).sum()} quark, {(predictions == 0).sum()} gluon\")\n",
    "print(f\"True distribution: {(y_eval == 1).sum()} quark, {(y_eval == 0).sum()} gluon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_eval, predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                Predicted\")\n",
    "print(\"                Gluon  Quark\")\n",
    "print(f\"True  Gluon     {cm[0,0]:5d}  {cm[0,1]:5d}\")\n",
    "print(f\"      Quark     {cm[1,0]:5d}  {cm[1,1]:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Gluon (0)', 'Quark (1)'],\n",
    "            yticklabels=['Gluon (0)', 'Quark (1)'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title(f'Confusion Matrix - Local LLM Classifier\\nAccuracy: {accuracy:.3f}, AUC: {auc:.3f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary\n",
    "\n",
    "Key takeaways from the local LLM classifier:\n",
    "\n",
    "1. **Async Processing**: Significantly faster than sequential (typically 5-10x speedup)\n",
    "2. **Reasoning Effort**: Higher effort levels use more tokens but may improve accuracy\n",
    "3. **Local Deployment**: No API costs, full control over infrastructure\n",
    "4. **Zero-Shot**: No training required, just prompt engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
