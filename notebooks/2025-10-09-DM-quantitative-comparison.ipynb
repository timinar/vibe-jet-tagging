{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantitative Comparison: Feature-Based vs Raw Particles\n",
        "\n",
        "This notebook performs a rigorous quantitative comparison of different template approaches on 100 jets.\n",
        "\n",
        "**Test Conditions:**\n",
        "- Sample size: 100 jets (statistically significant)\n",
        "- Async batching: batch_size=10 for speed\n",
        "- Thinking budget: 1000 tokens  \n",
        "- Templates tested:\n",
        "  1. Raw particles (simple_list)\n",
        "  2. Features only (features_basic, features_kinematic, features_full)\n",
        "  3. Hybrid (hybrid_basic, hybrid_kinematic, hybrid_full)\n",
        "\n",
        "**Metrics:**\n",
        "- Accuracy\n",
        "- AUC (Area Under ROC Curve)\n",
        "- Total cost ($)\n",
        "- Average time per jet\n",
        "- Token efficiency (tokens/jet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
        "\n",
        "from vibe_jet_tagging import LLMClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Test Data (100 jets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: 100 jets\n",
            "  Quark: 50 (50.0%)\n",
            "  Gluon: 50 (50.0%)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "data_path = Path.cwd().parent / 'data' / 'qg_jets.npz'\n",
        "data = np.load(data_path)\n",
        "\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "\n",
        "# Take 100 test jets (balanced)\n",
        "np.random.seed(42)\n",
        "quark_idx = np.where(y == 1)[0]\n",
        "gluon_idx = np.where(y == 0)[0]\n",
        "\n",
        "# Sample 50 of each\n",
        "test_quark = np.random.choice(quark_idx, 50, replace=False)\n",
        "test_gluon = np.random.choice(gluon_idx, 50, replace=False)\n",
        "test_idx = np.concatenate([test_quark, test_gluon])\n",
        "np.random.shuffle(test_idx)\n",
        "\n",
        "X_test = X[test_idx]\n",
        "y_test = y[test_idx]\n",
        "\n",
        "print(f\"Test set: {len(X_test)} jets\")\n",
        "print(f\"  Quark: {np.sum(y_test == 1)} ({np.mean(y_test)*100:.1f}%)\")\n",
        "print(f\"  Gluon: {np.sum(y_test == 0)} ({(1-np.mean(y_test))*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Template Configurations\n",
        "\n",
        "We'll test 7 templates systematically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing 3 configurations\n",
            "  - Raw: Simple List\n",
            "  - Features: Full\n",
            "  - Hybrid: Full\n"
          ]
        }
      ],
      "source": [
        "configs = [\n",
        "    (\"Raw: Simple List\", \"simple_list\", \"none\"),\n",
        "    # (\"Features: Basic\", \"features_basic\", None),\n",
        "    # (\"Features: Kinematic\", \"features_kinematic\", None),\n",
        "    (\"Features: Full\", \"features_full\", None),\n",
        "    # (\"Hybrid: Basic\", \"hybrid_basic\", None),\n",
        "    # (\"Hybrid: Kinematic\", \"hybrid_kinematic\", None),\n",
        "    (\"Hybrid: Full\", \"hybrid_full\", None),\n",
        "]\n",
        "\n",
        "print(f\"Testing {len(configs)} configurations\")\n",
        "for name, template, _ in configs:\n",
        "    print(f\"  - {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run Evaluation\n",
        "\n",
        "**Warning:** This will make ~100 API calls per template (~700 total calls).  \n",
        "Estimated cost: ~$0.50 total.  \n",
        "Estimated time: ~10-15 minutes with async batching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Testing: Raw: Simple List\n",
            "Template: simple_list\n",
            "======================================================================\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Time the prediction\u001b[39;00m\n\u001b[32m     20\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m predictions = \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Async batching\u001b[39;00m\n\u001b[32m     22\u001b[39m elapsed_time = time.time() - start_time\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/global/cfs/cdirs/m4958/usr/danieltm/Side_Work/FoundationModels/VibeJetTagging/vibe-jet-tagging/src/vibe_jet_tagging/llm_classifier.py:276\u001b[39m, in \u001b[36mLLMClassifier.predict\u001b[39m\u001b[34m(self, X, verbose, batch_size)\u001b[39m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mClassifier must be fitted before predict\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Use async batching\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# Sequential processing (original behavior)\u001b[39;00m\n\u001b[32m    279\u001b[39m predictions = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
            "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for name, template, extractor in configs:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Testing: {name}\")\n",
        "    print(f\"Template: {template}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    clf = LLMClassifier(\n",
        "        template_name=template,\n",
        "        templates_dir='../templates',\n",
        "        feature_extractor=extractor,\n",
        "        thinking_budget=1000,\n",
        "        max_tokens=2000,\n",
        "    )\n",
        "    \n",
        "    clf.fit([], [])\n",
        "    \n",
        "    # Time the prediction\n",
        "    start_time = time.time()\n",
        "    predictions = clf.predict(X_test, batch_size=10)  # Async batching\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Compute metrics\n",
        "    acc = accuracy_score(y_test, predictions)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test, predictions)\n",
        "    except:\n",
        "        auc = 0.5  # If all same class\n",
        "    \n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "    \n",
        "    # Token stats\n",
        "    total_tokens = clf.total_prompt_tokens + clf.total_completion_tokens + clf.total_thinking_tokens\n",
        "    tokens_per_jet = total_tokens / len(X_test)\n",
        "    time_per_jet = elapsed_time / len(X_test)\n",
        "    \n",
        "    results.append({\n",
        "        'name': name,\n",
        "        'template': template,\n",
        "        'accuracy': acc,\n",
        "        'auc': auc,\n",
        "        'cost': clf.total_cost,\n",
        "        'total_time': elapsed_time,\n",
        "        'time_per_jet': time_per_jet,\n",
        "        'prompt_tokens': clf.total_prompt_tokens,\n",
        "        'completion_tokens': clf.total_completion_tokens,\n",
        "        'thinking_tokens': clf.total_thinking_tokens,\n",
        "        'total_tokens': total_tokens,\n",
        "        'tokens_per_jet': tokens_per_jet,\n",
        "        'confusion_matrix': cm,\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n Results:\")\n",
        "    print(f\"  Accuracy: {acc:.3f}\")\n",
        "    print(f\"  AUC: {auc:.3f}\")\n",
        "    print(f\"  Cost: ${clf.total_cost:.4f}\")\n",
        "    print(f\"  Time: {elapsed_time:.1f}s ({time_per_jet:.2f}s/jet)\")\n",
        "    print(f\"  Tokens: {total_tokens:,} ({tokens_per_jet:.1f}/jet)\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✓ All evaluations complete!\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Display main metrics\n",
        "display_df = df[['name', 'accuracy', 'auc', 'cost', 'tokens_per_jet', 'time_per_jet']].copy()\n",
        "display_df['cost'] = display_df['cost'].map('${:.4f}'.format)\n",
        "display_df['tokens_per_jet'] = display_df['tokens_per_jet'].map('{:.0f}'.format)\n",
        "display_df['time_per_jet'] = display_df['time_per_jet'].map('{:.2f}s'.format)\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"QUANTITATIVE RESULTS (100 jets)\")\n",
        "print(\"=\"*90)\n",
        "print(display_df.to_string(index=False))\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Find best performers\n",
        "best_acc_idx = df['accuracy'].idxmax()\n",
        "best_auc_idx = df['auc'].idxmax()\n",
        "best_cost_idx = df['cost'].idxmin()\n",
        "\n",
        "print(f\"\\n Best Accuracy: {df.loc[best_acc_idx, 'name']} ({df.loc[best_acc_idx, 'accuracy']:.3f})\")\n",
        "print(f\"Best AUC:      {df.loc[best_auc_idx, 'name']} ({df.loc[best_auc_idx, 'auc']:.3f})\")\n",
        "print(f\"Best Cost:     {df.loc[best_cost_idx, 'name']} (${df.loc[best_cost_idx, 'cost']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "names = df['name'].values\n",
        "colors = ['steelblue', 'coral', 'gold', 'lime', 'mediumseagreen', 'orchid', 'cyan']\n",
        "\n",
        "# Accuracy\n",
        "axes[0, 0].barh(names, df['accuracy'], color=colors)\n",
        "axes[0, 0].set_xlabel('Accuracy')\n",
        "axes[0, 0].set_title('Accuracy Comparison')\n",
        "axes[0, 0].axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
        "axes[0, 0].axvline(0.84, color='orange', linestyle='--', alpha=0.5, label='Baseline (mult cut)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# AUC\n",
        "axes[0, 1].barh(names, df['auc'], color=colors)\n",
        "axes[0, 1].set_xlabel('AUC')\n",
        "axes[0, 1].set_title('AUC Comparison')\n",
        "axes[0, 1].axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
        "axes[0, 1].axvline(0.84, color='orange', linestyle='--', alpha=0.5, label='Baseline')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Cost\n",
        "axes[1, 0].barh(names, df['cost'], color=colors)\n",
        "axes[1, 0].set_xlabel('Cost ($)')\n",
        "axes[1, 0].set_title('Cost per 100 Jets')\n",
        "axes[1, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Tokens per jet\n",
        "axes[1, 1].barh(names, df['tokens_per_jet'], color=colors)\n",
        "axes[1, 1].set_xlabel('Tokens per Jet')\n",
        "axes[1, 1].set_title('Token Efficiency')\n",
        "axes[1, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance vs Cost Trade-off\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot AUC vs Cost\n",
        "for i, row in df.iterrows():\n",
        "    plt.scatter(row['cost'], row['auc'], s=200, color=colors[i], alpha=0.7, edgecolors='black', linewidth=2)\n",
        "    plt.annotate(row['name'].split(':')[0], (row['cost'], row['auc']), \n",
        "                fontsize=9, ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel('Cost ($)', fontsize=12)\n",
        "plt.ylabel('AUC', fontsize=12)\n",
        "plt.title('Performance vs Cost Trade-off', fontsize=14, fontweight='bold')\n",
        "plt.axhline(0.84, color='orange', linestyle='--', alpha=0.5, label='Baseline AUC (0.84)')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Key Insights:\")\n",
        "print(\"  - Top-right corner = best (high AUC, low cost)\")\n",
        "print(\"  - Feature-only templates typically offer best cost efficiency\")\n",
        "print(\"  - Hybrid templates may provide marginal performance gains\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistical Summary\n",
        "\n",
        "Calculate statistical significance and confidence intervals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate binomial confidence intervals for accuracy\n",
        "from scipy import stats\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STATISTICAL ANALYSIS (95% Confidence Intervals)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    n = 100  # number of jets\n",
        "    p = row['accuracy']  # observed accuracy\n",
        "    \n",
        "    # Wilson score interval (better for edge cases)\n",
        "    ci_low, ci_high = stats.binom.interval(0.95, n, p)\n",
        "    ci_low /= n\n",
        "    ci_high /= n\n",
        "    \n",
        "    print(f\"\\n{row['name']}:\")\n",
        "    print(f\"  Accuracy: {p:.3f} [{ci_low:.3f}, {ci_high:.3f}]\")\n",
        "    print(f\"  AUC:      {row['auc']:.3f}\")\n",
        "    print(f\"  Cost:     ${row['cost']:.4f}\")\n",
        "    print(f\"  Tokens:   {row['tokens_per_jet']:.0f}/jet\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ Analysis complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
